{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final implementation of SLR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soedirman-Machine-Learning/experiment-repo/blob/master/final_implementation_of_SLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKpbOx93QiwK",
        "colab_type": "code",
        "outputId": "3f0279e5-307e-400a-ffa1-3613b8d4676a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818pV85FRYsj",
        "colab_type": "code",
        "outputId": "1a30a377-10db-4bd5-bc03-0299794dfa1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', input_shape=(224, 224, 3), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "\n",
        "#model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "#model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "#model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "#model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
        "#model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(8, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 200704)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                6422560   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 264       \n",
            "=================================================================\n",
            "Total params: 7,568,232\n",
            "Trainable params: 7,568,232\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiGg5wVkVBKw",
        "colab_type": "code",
        "outputId": "eaa7552b-3db3-449b-f758-80e8a030f15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4270
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from numpy import argmax\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from skimage import img_as_ubyte\n",
        "import skimage.transform\n",
        "import time\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, zoom_range=0.2, brightness_range=[0.8, 1.3], horizontal_flip=False)\n",
        " #Labels go here\n",
        "\n",
        "path1 = '/content/drive/My Drive/SLR/Images/' \n",
        "listing = os.listdir(path1)    \n",
        "count = 0\n",
        "\n",
        "\n",
        "for epoc in range(10):\n",
        "  S=[0,0,0,0,0,0,0,0]\n",
        "  X=[]  #Images goes here\n",
        "  Y=[] \n",
        "  print('ginga', epoc)\n",
        "  for file in listing:\n",
        "    #print(ord(file[3])-48,file[3])\n",
        "    count+=1\n",
        "    if ord(file[3])-48 == epoc:\n",
        "      #print(file)\n",
        "      S[ord(file[0])-65] += 1\n",
        "      image = cv2.imread(path1+file)\n",
        "\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      image = img_as_ubyte(skimage.transform.resize(image, (224, 224)))    \n",
        "      image=np.array(image)\n",
        "      image.astype(np.float32)\n",
        "      image = image/255\n",
        "      X.append(image)\n",
        "      Y.append(ord(file[0])-65)\n",
        "      start = time.time()\n",
        "\n",
        "      img = np.expand_dims(image, 0)\n",
        "      aug_itr = gen.flow(img)\n",
        "      aug_images = [next(aug_itr)[0].astype(np.float32) for i in range(35)]\n",
        "      for i in aug_images:\n",
        "        i = i/255\n",
        "        X.append(i)\n",
        "        Y.append(ord(file[0])-65)\n",
        "      end = time.time()\n",
        "      #print(len(Y),\" \",(end-start),Y[len(Y)-1])\n",
        "  print(S)\n",
        "  \n",
        "  X=np.array(X)\n",
        "  Y=np.array(Y)\n",
        "\n",
        "  Y = Y.reshape(-1,1)\n",
        "  Y = to_categorical(Y)\n",
        "\n",
        "  print(X.shape,Y.shape)\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "\n",
        "  model.compile(optimizer ='adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "  model.fit(x_train,y_train,validation_data=(x_test,y_test), verbose=1,epochs=10,batch_size=50,shuffle=True)\n",
        "  np.delete(X,X)\n",
        "  np.delete(X,X)\n",
        "#,callbacks=[monitor,checkpointer]\n",
        "tf.keras.models.save_model(model, '/content/my_model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ginga 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[5, 6, 5, 6, 8, 5, 8, 7]\n",
            "(1800, 224, 224, 3) (1800, 8)\n",
            "Train on 1620 samples, validate on 180 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "1620/1620 [==============================] - 48s 30ms/sample - loss: 2.3792 - acc: 0.1506 - val_loss: 2.0363 - val_acc: 0.2833\n",
            "Epoch 2/10\n",
            "1620/1620 [==============================] - 33s 21ms/sample - loss: 1.7604 - acc: 0.3179 - val_loss: 1.3394 - val_acc: 0.5944\n",
            "Epoch 3/10\n",
            "1620/1620 [==============================] - 33s 21ms/sample - loss: 1.4281 - acc: 0.4000 - val_loss: 1.1578 - val_acc: 0.5611\n",
            "Epoch 4/10\n",
            "1620/1620 [==============================] - 33s 20ms/sample - loss: 1.3105 - acc: 0.4506 - val_loss: 0.9790 - val_acc: 0.7000\n",
            "Epoch 5/10\n",
            "1620/1620 [==============================] - 33s 20ms/sample - loss: 1.2366 - acc: 0.4691 - val_loss: 0.8187 - val_acc: 0.6889\n",
            "Epoch 6/10\n",
            "1620/1620 [==============================] - 33s 21ms/sample - loss: 1.2558 - acc: 0.4512 - val_loss: 0.7051 - val_acc: 0.8056\n",
            "Epoch 7/10\n",
            "1620/1620 [==============================] - 33s 21ms/sample - loss: 1.1621 - acc: 0.4858 - val_loss: 0.6154 - val_acc: 0.7167\n",
            "Epoch 8/10\n",
            "1620/1620 [==============================] - 33s 20ms/sample - loss: 1.1466 - acc: 0.4920 - val_loss: 0.5011 - val_acc: 0.8333\n",
            "Epoch 9/10\n",
            "1620/1620 [==============================] - 33s 20ms/sample - loss: 1.0952 - acc: 0.5302 - val_loss: 0.6159 - val_acc: 0.7944\n",
            "Epoch 10/10\n",
            "1620/1620 [==============================] - 33s 21ms/sample - loss: 1.0315 - acc: 0.5488 - val_loss: 0.5695 - val_acc: 0.7833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ginga 1\n",
            "[5, 5, 5, 6, 7, 5, 8, 7]\n",
            "(1728, 224, 224, 3) (1728, 8)\n",
            "Train on 1555 samples, validate on 173 samples\n",
            "Epoch 1/10\n",
            "1555/1555 [==============================] - 35s 23ms/sample - loss: 1.2408 - acc: 0.4714 - val_loss: 0.7188 - val_acc: 0.8497\n",
            "Epoch 2/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 1.0968 - acc: 0.5190 - val_loss: 0.6595 - val_acc: 0.9249\n",
            "Epoch 3/10\n",
            "1555/1555 [==============================] - 32s 21ms/sample - loss: 1.0722 - acc: 0.5113 - val_loss: 0.4964 - val_acc: 0.9133\n",
            "Epoch 4/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 1.0801 - acc: 0.5151 - val_loss: 0.5462 - val_acc: 0.9191\n",
            "Epoch 5/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.9822 - acc: 0.5479 - val_loss: 0.4828 - val_acc: 0.9191\n",
            "Epoch 6/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 1.0149 - acc: 0.5498 - val_loss: 0.5196 - val_acc: 0.9191\n",
            "Epoch 7/10\n",
            "1555/1555 [==============================] - 32s 21ms/sample - loss: 0.9774 - acc: 0.5730 - val_loss: 0.4274 - val_acc: 0.8960\n",
            "Epoch 8/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 1.0156 - acc: 0.5395 - val_loss: 0.4644 - val_acc: 0.9191\n",
            "Epoch 9/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.9568 - acc: 0.5588 - val_loss: 0.5534 - val_acc: 0.9249\n",
            "Epoch 10/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.9627 - acc: 0.5601 - val_loss: 0.4953 - val_acc: 0.9133\n",
            "ginga 2\n",
            "[5, 5, 5, 6, 7, 5, 7, 7]\n",
            "(1692, 224, 224, 3) (1692, 8)\n",
            "Train on 1522 samples, validate on 170 samples\n",
            "Epoch 1/10\n",
            "1522/1522 [==============================] - 36s 23ms/sample - loss: 1.0436 - acc: 0.5604 - val_loss: 0.6197 - val_acc: 0.8294\n",
            "Epoch 2/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.9832 - acc: 0.5637 - val_loss: 0.4366 - val_acc: 0.9118\n",
            "Epoch 3/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.9783 - acc: 0.5552 - val_loss: 0.4261 - val_acc: 0.8824\n",
            "Epoch 4/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.9306 - acc: 0.5959 - val_loss: 0.3903 - val_acc: 0.8588\n",
            "Epoch 5/10\n",
            "1522/1522 [==============================] - 32s 21ms/sample - loss: 0.8872 - acc: 0.5854 - val_loss: 0.4470 - val_acc: 0.9353\n",
            "Epoch 6/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.9024 - acc: 0.5972 - val_loss: 0.3604 - val_acc: 0.8941\n",
            "Epoch 7/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.8768 - acc: 0.6012 - val_loss: 0.3566 - val_acc: 0.9176\n",
            "Epoch 8/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.8723 - acc: 0.5999 - val_loss: 0.3496 - val_acc: 0.9235\n",
            "Epoch 9/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.8737 - acc: 0.6032 - val_loss: 0.3733 - val_acc: 0.9353\n",
            "Epoch 10/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.8562 - acc: 0.6097 - val_loss: 0.3073 - val_acc: 0.9471\n",
            "ginga 3\n",
            "[5, 5, 5, 6, 8, 5, 7, 7]\n",
            "(1728, 224, 224, 3) (1728, 8)\n",
            "Train on 1555 samples, validate on 173 samples\n",
            "Epoch 1/10\n",
            "1555/1555 [==============================] - 32s 21ms/sample - loss: 0.9211 - acc: 0.5949 - val_loss: 0.5039 - val_acc: 0.8266\n",
            "Epoch 2/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.9552 - acc: 0.5929 - val_loss: 0.3458 - val_acc: 0.9364\n",
            "Epoch 3/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.8788 - acc: 0.6386 - val_loss: 0.3081 - val_acc: 0.9538\n",
            "Epoch 4/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.8039 - acc: 0.6412 - val_loss: 0.2909 - val_acc: 0.9711\n",
            "Epoch 5/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.8231 - acc: 0.6257 - val_loss: 0.2548 - val_acc: 0.9595\n",
            "Epoch 6/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.7634 - acc: 0.6553 - val_loss: 0.2546 - val_acc: 0.9480\n",
            "Epoch 7/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.8109 - acc: 0.6424 - val_loss: 0.2596 - val_acc: 0.9711\n",
            "Epoch 8/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.7992 - acc: 0.6489 - val_loss: 0.3221 - val_acc: 0.9711\n",
            "Epoch 9/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.7426 - acc: 0.6707 - val_loss: 0.3084 - val_acc: 0.9422\n",
            "Epoch 10/10\n",
            "1555/1555 [==============================] - 32s 20ms/sample - loss: 0.7296 - acc: 0.6913 - val_loss: 0.2259 - val_acc: 0.9653\n",
            "ginga 4\n",
            "[5, 5, 5, 6, 7, 5, 7, 7]\n",
            "(1692, 224, 224, 3) (1692, 8)\n",
            "Train on 1522 samples, validate on 170 samples\n",
            "Epoch 1/10\n",
            "1522/1522 [==============================] - 32s 21ms/sample - loss: 0.7721 - acc: 0.6800 - val_loss: 0.3075 - val_acc: 0.9294\n",
            "Epoch 2/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.6930 - acc: 0.7063 - val_loss: 0.2695 - val_acc: 0.9706\n",
            "Epoch 3/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.6841 - acc: 0.7135 - val_loss: 0.1309 - val_acc: 0.9824\n",
            "Epoch 4/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.6298 - acc: 0.7359 - val_loss: 0.1453 - val_acc: 0.9941\n",
            "Epoch 5/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.6712 - acc: 0.7135 - val_loss: 0.1345 - val_acc: 0.9765\n",
            "Epoch 6/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.6184 - acc: 0.7339 - val_loss: 0.1603 - val_acc: 0.9882\n",
            "Epoch 7/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.6137 - acc: 0.7405 - val_loss: 0.1610 - val_acc: 0.9941\n",
            "Epoch 8/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.6126 - acc: 0.7346 - val_loss: 0.1709 - val_acc: 0.9824\n",
            "Epoch 9/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.5550 - acc: 0.7530 - val_loss: 0.1058 - val_acc: 0.9941\n",
            "Epoch 10/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.5579 - acc: 0.7727 - val_loss: 0.1878 - val_acc: 0.9588\n",
            "ginga 5\n",
            "[5, 5, 5, 6, 7, 5, 7, 7]\n",
            "(1692, 224, 224, 3) (1692, 8)\n",
            "Train on 1522 samples, validate on 170 samples\n",
            "Epoch 1/10\n",
            "1522/1522 [==============================] - 32s 21ms/sample - loss: 0.6950 - acc: 0.7221 - val_loss: 0.1992 - val_acc: 0.9529\n",
            "Epoch 2/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.6406 - acc: 0.7516 - val_loss: 0.2363 - val_acc: 0.9294\n",
            "Epoch 3/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.6043 - acc: 0.7424 - val_loss: 0.1742 - val_acc: 0.9412\n",
            "Epoch 4/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.5437 - acc: 0.7740 - val_loss: 0.1587 - val_acc: 0.9588\n",
            "Epoch 5/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.5423 - acc: 0.7819 - val_loss: 0.1092 - val_acc: 0.9706\n",
            "Epoch 6/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.5616 - acc: 0.7700 - val_loss: 0.1523 - val_acc: 0.9471\n",
            "Epoch 7/10\n",
            "1522/1522 [==============================] - 31s 21ms/sample - loss: 0.5655 - acc: 0.7654 - val_loss: 0.1113 - val_acc: 0.9765\n",
            "Epoch 8/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.5125 - acc: 0.7806 - val_loss: 0.1735 - val_acc: 0.9412\n",
            "Epoch 9/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.5264 - acc: 0.7654 - val_loss: 0.1028 - val_acc: 0.9706\n",
            "Epoch 10/10\n",
            "1522/1522 [==============================] - 31s 20ms/sample - loss: 0.4621 - acc: 0.7950 - val_loss: 0.1191 - val_acc: 0.9588\n",
            "ginga 6\n",
            "[5, 5, 5, 5, 7, 5, 7, 7]\n",
            "(1656, 224, 224, 3) (1656, 8)\n",
            "Train on 1490 samples, validate on 166 samples\n",
            "Epoch 1/10\n",
            "1490/1490 [==============================] - 38s 26ms/sample - loss: 0.5214 - acc: 0.7866 - val_loss: 0.1131 - val_acc: 0.9940\n",
            "Epoch 2/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4922 - acc: 0.7879 - val_loss: 0.0810 - val_acc: 0.9940\n",
            "Epoch 3/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4874 - acc: 0.7886 - val_loss: 0.0845 - val_acc: 0.9880\n",
            "Epoch 4/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4363 - acc: 0.8181 - val_loss: 0.0810 - val_acc: 0.9880\n",
            "Epoch 5/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4614 - acc: 0.8054 - val_loss: 0.0499 - val_acc: 0.9940\n",
            "Epoch 6/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4181 - acc: 0.8208 - val_loss: 0.0520 - val_acc: 0.9940\n",
            "Epoch 7/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4292 - acc: 0.8094 - val_loss: 0.0679 - val_acc: 0.9880\n",
            "Epoch 8/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4083 - acc: 0.8128 - val_loss: 0.0920 - val_acc: 0.9759\n",
            "Epoch 9/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4210 - acc: 0.8114 - val_loss: 0.0972 - val_acc: 0.9880\n",
            "Epoch 10/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.3827 - acc: 0.8282 - val_loss: 0.0420 - val_acc: 0.9940\n",
            "ginga 7\n",
            "[5, 5, 5, 5, 7, 5, 7, 7]\n",
            "(1656, 224, 224, 3) (1656, 8)\n",
            "Train on 1490 samples, validate on 166 samples\n",
            "Epoch 1/10\n",
            "1490/1490 [==============================] - 31s 21ms/sample - loss: 0.5477 - acc: 0.7913 - val_loss: 0.1254 - val_acc: 0.9699\n",
            "Epoch 2/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.5042 - acc: 0.7960 - val_loss: 0.1273 - val_acc: 0.9699\n",
            "Epoch 3/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4970 - acc: 0.8047 - val_loss: 0.1365 - val_acc: 0.9639\n",
            "Epoch 4/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.5084 - acc: 0.7826 - val_loss: 0.1316 - val_acc: 0.9699\n",
            "Epoch 5/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4802 - acc: 0.7872 - val_loss: 0.1376 - val_acc: 0.9578\n",
            "Epoch 6/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4648 - acc: 0.8107 - val_loss: 0.1124 - val_acc: 0.9699\n",
            "Epoch 7/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4308 - acc: 0.8121 - val_loss: 0.1440 - val_acc: 0.9639\n",
            "Epoch 8/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4691 - acc: 0.8020 - val_loss: 0.1088 - val_acc: 0.9699\n",
            "Epoch 9/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4457 - acc: 0.8168 - val_loss: 0.1180 - val_acc: 0.9699\n",
            "Epoch 10/10\n",
            "1490/1490 [==============================] - 30s 20ms/sample - loss: 0.4371 - acc: 0.8208 - val_loss: 0.1414 - val_acc: 0.9398\n",
            "ginga 8\n",
            "[5, 5, 4, 5, 7, 5, 7, 7]\n",
            "(1620, 224, 224, 3) (1620, 8)\n",
            "Train on 1458 samples, validate on 162 samples\n",
            "Epoch 1/10\n",
            "1458/1458 [==============================] - 33s 23ms/sample - loss: 0.4434 - acc: 0.8258 - val_loss: 0.1045 - val_acc: 0.9815\n",
            "Epoch 2/10\n",
            "1458/1458 [==============================] - 30s 21ms/sample - loss: 0.4354 - acc: 0.8210 - val_loss: 0.0507 - val_acc: 0.9938\n",
            "Epoch 3/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.3519 - acc: 0.8690 - val_loss: 0.0859 - val_acc: 0.9815\n",
            "Epoch 4/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.4510 - acc: 0.8169 - val_loss: 0.0391 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.3952 - acc: 0.8443 - val_loss: 0.0387 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.3390 - acc: 0.8642 - val_loss: 0.0219 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.2950 - acc: 0.8813 - val_loss: 0.0104 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.2798 - acc: 0.8800 - val_loss: 0.0190 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.2966 - acc: 0.8772 - val_loss: 0.0394 - val_acc: 0.9938\n",
            "Epoch 10/10\n",
            "1458/1458 [==============================] - 30s 20ms/sample - loss: 0.3120 - acc: 0.8642 - val_loss: 0.0082 - val_acc: 1.0000\n",
            "ginga 9\n",
            "[4, 5, 4, 5, 7, 5, 7, 7]\n",
            "(1584, 224, 224, 3) (1584, 8)\n",
            "Train on 1425 samples, validate on 159 samples\n",
            "Epoch 1/10\n",
            "1425/1425 [==============================] - 35s 25ms/sample - loss: 0.3061 - acc: 0.8751 - val_loss: 0.0133 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.3094 - acc: 0.8744 - val_loss: 0.0192 - val_acc: 0.9937\n",
            "Epoch 3/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.3475 - acc: 0.8681 - val_loss: 0.0299 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2743 - acc: 0.8856 - val_loss: 0.0141 - val_acc: 0.9937\n",
            "Epoch 5/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2876 - acc: 0.8814 - val_loss: 0.0130 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2448 - acc: 0.8982 - val_loss: 0.0056 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2571 - acc: 0.8975 - val_loss: 0.0158 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2523 - acc: 0.8996 - val_loss: 0.0080 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2632 - acc: 0.9053 - val_loss: 0.0234 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "1425/1425 [==============================] - 29s 20ms/sample - loss: 0.2905 - acc: 0.8891 - val_loss: 0.0183 - val_acc: 0.9937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOfxDQPUQys9",
        "colab_type": "code",
        "outputId": "c896d6d4-4592-44b2-cc36-229a7772c65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Flatten,Dense,Input,AveragePooling2D\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import numpy\n",
        "from keras.applications import mobilenet \n",
        "from skimage import img_as_ubyte\n",
        "import skimage.transform\n",
        "import tensorflow as tf\n",
        "\n",
        "model2 = tf.keras.models.load_model(\"/content/my_model.h5\")\n",
        "\n",
        "pred = model2.predict(x_test)\n",
        "su=0 \n",
        "for i in range (len(x_test)):\n",
        "  if np.argmax(pred[i]) == np.argmax(y_test[i]):\n",
        "    su+=1\n",
        "print(su/len(y_test))\n",
        "    \n",
        "  \n",
        "  #print(chr(pred[i]+65),chr(np.argmax(y_test[i])+65))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.996742671009772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dkUBE6pQqlF",
        "colab_type": "code",
        "outputId": "95af13b1-8277-41df-d928-90a715015da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from tensorflow.contrib import lite\n",
        "converter = lite.TFLiteConverter.from_keras_model_file( '/content/my_model.h5' ) # Your model's name\n",
        "model = converter.convert()\n",
        "file = open( '/content/model10.tflite' , 'wb' ) \n",
        "file.write( model )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n",
            "INFO:tensorflow:Froze 16 variables.\n",
            "INFO:tensorflow:Converted 16 variables to const ops.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30276464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y1iw1E6ODPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApteG0hj35tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#below codes are obsolete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPh1r7On35wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZMCKB3_35y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AW5eZyj351V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2oo3sgy353T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFNhgbu1356K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsbfvjdYPUC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpeqsJbEQD6s",
        "colab_type": "code",
        "outputId": "0fa18ca9-8852-40f6-82be-59871695fabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9404
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from numpy import argmax\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from skimage import img_as_ubyte\n",
        "import skimage.transform\n",
        "import time\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, zoom_range=0.2, brightness_range=[0.8, 1.3], horizontal_flip=False)\n",
        " #Labels go here\n",
        "S=[]\n",
        "X=[]  #Images goes here\n",
        "Y=[] \n",
        "path1 = '/content/drive/My Drive/SLR/Images/' \n",
        "listing = os.listdir(path1)    \n",
        "count = 0\n",
        "for file in listing:\n",
        " \n",
        "  count+=1\n",
        "  if file.endswith('.jpg'):\n",
        "    image = cv2.imread(path1+file)\n",
        "    #print(file)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    #image.resize(224,224,3)\n",
        "    #image.reshape(224,224,3)\n",
        "    image = img_as_ubyte(skimage.transform.resize(image, (224, 224)))\n",
        "    #cv2.resize(image,(224,224))\n",
        "    #plot = plt.imshow(image)\n",
        "    \n",
        "    \n",
        "\n",
        "    #image = image.astype(np.uint8)\n",
        "    #S.append(image)\n",
        "    \n",
        "    image=np.array(image)\n",
        "    image.astype(np.float32)\n",
        "    image = image/255\n",
        "    X.append(image)\n",
        "    Y.append(ord(file[0])-65)\n",
        "    start = time.time()\n",
        "\n",
        "    img = np.expand_dims(image, 0)\n",
        "    aug_itr = gen.flow(img)\n",
        "    aug_images = [next(aug_itr)[0].astype(np.float32) for i in range(5)]\n",
        "    for i in aug_images:\n",
        "      #i = i.astype(np.uint8)\n",
        "      #i = img_as_ubyte(skimage.transform.resize(i, (224, 224)))\n",
        "      #i = i.astype(np.float32)\n",
        "      i = i/255\n",
        "      X.append(i)\n",
        "      Y.append(ord(file[0])-65)\n",
        "    end = time.time()\n",
        "    print(len(Y),\" \",(end-start),Y[len(Y)-1])\n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n",
        "\n",
        "#print(model.layers[len(mobile_model.layers)])\n",
        "\n",
        "Y = Y.reshape(-1,1)\n",
        "Y = to_categorical(Y)\n",
        "\n",
        "print(X.shape,Y.shape)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer ='adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test), verbose=1,epochs=10,batch_size=50,shuffle=True) \n",
        "#,callbacks=[monitor,checkpointer]\n",
        "tf.keras.models.save_model(model, '/content/my_model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9   0.13520288467407227 7\n",
            "18   0.1335277557373047 7\n",
            "27   0.12703371047973633 4\n",
            "36   0.135528564453125 5\n",
            "45   0.14671802520751953 5\n",
            "54   0.13726401329040527 4\n",
            "63   0.14076995849609375 0\n",
            "72   0.15035676956176758 6\n",
            "81   0.12377429008483887 2\n",
            "90   0.14816617965698242 2\n",
            "99   0.15196990966796875 6\n",
            "108   0.14446401596069336 0\n",
            "117   0.14791226387023926 6\n",
            "126   0.15519928932189941 4\n",
            "135   0.1421213150024414 5\n",
            "144   0.14244318008422852 5\n",
            "153   0.15189862251281738 7\n",
            "162   0.14565181732177734 4\n",
            "171   0.15041041374206543 7\n",
            "180   0.1423354148864746 4\n",
            "189   0.14599251747131348 7\n",
            "198   0.1377730369567871 7\n",
            "207   0.15798044204711914 0\n",
            "216   0.1567249298095703 5\n",
            "225   0.1614058017730713 7\n",
            "234   0.1525719165802002 5\n",
            "243   0.15891575813293457 4\n",
            "252   0.153458833694458 6\n",
            "261   0.1518247127532959 4\n",
            "270   0.15167999267578125 6\n",
            "279   0.1534438133239746 0\n",
            "288   0.1549227237701416 6\n",
            "297   0.15840411186218262 2\n",
            "306   0.15149831771850586 2\n",
            "315   0.15197372436523438 6\n",
            "324   0.15957117080688477 0\n",
            "333   0.14923954010009766 6\n",
            "342   0.14566302299499512 4\n",
            "351   0.14537906646728516 6\n",
            "360   0.1784350872039795 4\n",
            "369   0.14693737030029297 5\n",
            "378   0.14873266220092773 7\n",
            "387   0.16340160369873047 0\n",
            "396   0.14953970909118652 5\n",
            "405   0.14893436431884766 7\n",
            "414   0.1588428020477295 7\n",
            "423   0.16266727447509766 7\n",
            "432   0.14378881454467773 5\n",
            "441   0.14980792999267578 5\n",
            "450   0.1581575870513916 7\n",
            "459   0.1528921127319336 4\n",
            "468   0.14470267295837402 7\n",
            "477   0.1356673240661621 4\n",
            "486   0.1595757007598877 4\n",
            "495   0.14521145820617676 6\n",
            "504   0.14505696296691895 6\n",
            "513   0.15064311027526855 6\n",
            "522   0.15393471717834473 2\n",
            "531   0.15370965003967285 2\n",
            "540   0.14180636405944824 2\n",
            "549   0.16009736061096191 2\n",
            "558   0.14388537406921387 0\n",
            "567   0.13803791999816895 6\n",
            "576   0.14513349533081055 6\n",
            "585   0.14734268188476562 6\n",
            "594   0.14116621017456055 4\n",
            "603   0.14336252212524414 4\n",
            "612   0.14629888534545898 7\n",
            "621   0.1564500331878662 7\n",
            "630   0.15624475479125977 4\n",
            "639   0.1712512969970703 0\n",
            "648   0.1759488582611084 5\n",
            "657   0.1690981388092041 5\n",
            "666   0.16813254356384277 7\n",
            "675   0.15396761894226074 5\n",
            "684   0.1750023365020752 5\n",
            "693   0.16353392601013184 7\n",
            "702   0.14892935752868652 4\n",
            "711   0.14598512649536133 7\n",
            "720   0.1633141040802002 4\n",
            "729   0.15900325775146484 6\n",
            "738   0.16083955764770508 0\n",
            "747   0.15642237663269043 6\n",
            "756   0.15851569175720215 2\n",
            "765   0.15601539611816406 6\n",
            "774   0.16748523712158203 0\n",
            "783   0.16266894340515137 6\n",
            "792   0.1627962589263916 4\n",
            "801   0.17037582397460938 4\n",
            "810   0.1715855598449707 7\n",
            "819   0.15180397033691406 7\n",
            "828   0.15706777572631836 5\n",
            "837   0.16037774085998535 5\n",
            "846   0.1655278205871582 7\n",
            "855   0.17699003219604492 0\n",
            "864   0.17771363258361816 7\n",
            "873   0.17413806915283203 7\n",
            "882   0.15918350219726562 5\n",
            "891   0.16645073890686035 0\n",
            "900   0.159804105758667 5\n",
            "909   0.16043663024902344 4\n",
            "918   0.15743565559387207 4\n",
            "927   0.1637895107269287 4\n",
            "936   0.15793943405151367 5\n",
            "945   0.17449688911437988 6\n",
            "954   0.15967273712158203 6\n",
            "963   0.1585848331451416 6\n",
            "972   0.1637248992919922 2\n",
            "981   0.15749645233154297 2\n",
            "990   0.15963101387023926 2\n",
            "999   0.14535808563232422 2\n",
            "1008   0.15059256553649902 6\n",
            "1017   0.1527419090270996 6\n",
            "1026   0.14775681495666504 5\n",
            "1035   0.1467421054840088 6\n",
            "1044   0.15050172805786133 4\n",
            "1053   0.1457517147064209 4\n",
            "1062   0.14203691482543945 4\n",
            "1071   0.1446688175201416 0\n",
            "1080   0.13720011711120605 5\n",
            "1089   0.1609201431274414 5\n",
            "1098   0.1779155731201172 7\n",
            "1107   0.14823222160339355 0\n",
            "1116   0.1409165859222412 7\n",
            "1125   0.1411142349243164 7\n",
            "1134   0.1459810733795166 7\n",
            "1143   0.13948798179626465 0\n",
            "1152   0.15468215942382812 7\n",
            "1161   0.15273070335388184 5\n",
            "1170   0.1535799503326416 5\n",
            "1179   0.1519176959991455 4\n",
            "1188   0.15152406692504883 4\n",
            "1197   0.1575460433959961 6\n",
            "1206   0.15134620666503906 6\n",
            "1215   0.15693068504333496 2\n",
            "1224   0.15935778617858887 2\n",
            "1233   0.16704154014587402 6\n",
            "1242   0.1568899154663086 5\n",
            "1251   0.15643000602722168 6\n",
            "1260   0.1555619239807129 4\n",
            "1269   0.1617603302001953 4\n",
            "1278   0.15738773345947266 5\n",
            "1287   0.15468192100524902 5\n",
            "1296   0.15633535385131836 7\n",
            "1305   0.16172266006469727 7\n",
            "1314   0.10554337501525879 0\n",
            "1323   0.10450935363769531 5\n",
            "1332   0.10095620155334473 5\n",
            "1341   0.09464573860168457 0\n",
            "1350   0.10056090354919434 7\n",
            "1359   0.1005713939666748 7\n",
            "1368   0.0968177318572998 4\n",
            "1377   0.10189032554626465 4\n",
            "1386   0.10167622566223145 6\n",
            "1395   0.1017916202545166 5\n",
            "1404   0.10787463188171387 6\n",
            "1413   0.10227560997009277 2\n",
            "1422   0.0965421199798584 2\n",
            "1431   0.09913039207458496 5\n",
            "1440   0.13996624946594238 6\n",
            "1449   0.1537308692932129 6\n",
            "1458   0.14167428016662598 4\n",
            "1467   0.1480271816253662 4\n",
            "1476   0.13895130157470703 4\n",
            "1485   0.14482903480529785 7\n",
            "1494   0.15341973304748535 0\n",
            "1503   0.1437976360321045 7\n",
            "1512   0.14330220222473145 5\n",
            "1521   0.14000844955444336 5\n",
            "1530   0.1419696807861328 0\n",
            "1539   0.15763616561889648 5\n",
            "1548   0.14017343521118164 5\n",
            "1557   0.15627312660217285 7\n",
            "1566   0.15867900848388672 7\n",
            "1575   0.13379430770874023 5\n",
            "1584   0.14189863204956055 0\n",
            "1593   0.15291261672973633 4\n",
            "1602   0.14556193351745605 4\n",
            "1611   0.14014625549316406 4\n",
            "1620   0.14807367324829102 6\n",
            "1629   0.1418142318725586 6\n",
            "1638   0.14368343353271484 5\n",
            "1647   0.13782715797424316 6\n",
            "1656   0.14421629905700684 7\n",
            "1665   0.1539771556854248 2\n",
            "1674   0.13681912422180176 2\n",
            "1683   0.14336419105529785 2\n",
            "1692   0.14140057563781738 2\n",
            "1701   0.14486455917358398 7\n",
            "1710   0.14251327514648438 6\n",
            "1719   0.15293216705322266 6\n",
            "1728   0.14370393753051758 5\n",
            "1737   0.1580054759979248 6\n",
            "1746   0.14049696922302246 4\n",
            "1755   0.13474464416503906 4\n",
            "1764   0.1317286491394043 4\n",
            "1773   0.134932279586792 5\n",
            "1782   0.15083956718444824 7\n",
            "1791   0.14711570739746094 0\n",
            "1800   0.15010619163513184 7\n",
            "1809   0.14754962921142578 5\n",
            "1818   0.14592957496643066 0\n",
            "1827   0.1440892219543457 5\n",
            "1836   0.14546704292297363 5\n",
            "1845   0.15614700317382812 0\n",
            "1854   0.15319371223449707 0\n",
            "1863   0.1554393768310547 7\n",
            "1872   0.15165281295776367 7\n",
            "1881   0.1460268497467041 5\n",
            "1890   0.149064302444458 4\n",
            "1899   0.14610648155212402 4\n",
            "1908   0.15058469772338867 4\n",
            "1917   0.145416259765625 6\n",
            "1926   0.14316487312316895 6\n",
            "1935   0.14429545402526855 6\n",
            "1944   0.14875245094299316 7\n",
            "1953   0.15062952041625977 2\n",
            "1962   0.13734173774719238 2\n",
            "1971   0.15572047233581543 2\n",
            "1980   0.14824938774108887 2\n",
            "1989   0.14816713333129883 2\n",
            "1998   0.15048980712890625 2\n",
            "2007   0.14506006240844727 7\n",
            "2016   0.15520787239074707 6\n",
            "2025   0.1467268466949463 6\n",
            "2034   0.14940285682678223 6\n",
            "2043   0.1395409107208252 4\n",
            "2052   0.1429309844970703 4\n",
            "2061   0.14351868629455566 4\n",
            "2070   0.14989018440246582 5\n",
            "2079   0.1398448944091797 7\n",
            "2088   0.14890527725219727 0\n",
            "2097   0.14922833442687988 7\n",
            "2106   0.13985133171081543 5\n",
            "2115   0.14776158332824707 0\n",
            "2124   0.13982248306274414 7\n",
            "2133   0.15602350234985352 0\n",
            "2142   0.15485882759094238 0\n",
            "2151   0.14781737327575684 5\n",
            "2160   0.1521928310394287 4\n",
            "2169   0.14810991287231445 4\n",
            "2178   0.15398883819580078 6\n",
            "2187   0.1441183090209961 6\n",
            "2196   0.14525961875915527 5\n",
            "2205   0.14986062049865723 6\n",
            "2214   0.1492750644683838 7\n",
            "2223   0.15331768989562988 2\n",
            "2232   0.1444242000579834 2\n",
            "2241   0.14310026168823242 2\n",
            "2250   0.14147305488586426 2\n",
            "2259   0.14168214797973633 7\n",
            "2268   0.14204788208007812 6\n",
            "2277   0.13956880569458008 5\n",
            "2286   0.14445185661315918 6\n",
            "2295   0.1424407958984375 6\n",
            "2304   0.14146876335144043 4\n",
            "2313   0.1391918659210205 4\n",
            "2322   0.139573335647583 5\n",
            "2331   0.14570236206054688 0\n",
            "2340   0.1461505889892578 7\n",
            "2349   0.15074801445007324 0\n",
            "2358   0.1478896141052246 5\n",
            "2367   0.15015196800231934 0\n",
            "2376   0.1517035961151123 7\n",
            "2385   0.1470630168914795 0\n",
            "2394   0.15358591079711914 4\n",
            "2403   0.14685940742492676 4\n",
            "2412   0.1528005599975586 4\n",
            "2421   0.1454944610595703 6\n",
            "2430   0.1555461883544922 6\n",
            "2439   0.14908814430236816 7\n",
            "2448   0.1503620147705078 2\n",
            "2457   0.156477689743042 2\n",
            "2466   0.149155855178833 2\n",
            "2475   0.15094900131225586 2\n",
            "2484   0.16000080108642578 7\n",
            "2493   0.1553032398223877 6\n",
            "2502   0.16874480247497559 6\n",
            "2511   0.1509087085723877 4\n",
            "2520   0.1540076732635498 4\n",
            "2529   0.1607651710510254 0\n",
            "2538   0.13707852363586426 0\n",
            "2547   0.14322304725646973 7\n",
            "2556   0.1408247947692871 5\n",
            "2565   0.14878439903259277 0\n",
            "2574   0.14319062232971191 5\n",
            "2583   0.1477675437927246 7\n",
            "2592   0.14525771141052246 0\n",
            "2601   0.14401817321777344 7\n",
            "2610   0.14147710800170898 5\n",
            "2619   0.14392566680908203 0\n",
            "2628   0.15816092491149902 4\n",
            "2637   0.14632272720336914 4\n",
            "2646   0.1437990665435791 4\n",
            "2655   0.14736247062683105 6\n",
            "2664   0.14759421348571777 6\n",
            "2673   0.14667463302612305 6\n",
            "2682   0.1421527862548828 2\n",
            "2691   0.15314722061157227 2\n",
            "2700   0.14307188987731934 2\n",
            "2709   0.14615535736083984 2\n",
            "2718   0.14758634567260742 7\n",
            "2727   0.1444084644317627 6\n",
            "2736   0.1412808895111084 6\n",
            "2745   0.14358758926391602 6\n",
            "2754   0.14245033264160156 4\n",
            "2763   0.13874220848083496 4\n",
            "2772   0.13300848007202148 4\n",
            "2781   0.1382005214691162 5\n",
            "2790   0.1419830322265625 7\n",
            "2799   0.14739608764648438 0\n",
            "2808   0.14190435409545898 0\n",
            "2817   0.14719772338867188 7\n",
            "2826   0.14947175979614258 0\n",
            "2835   0.14262604713439941 5\n",
            "2844   0.15022730827331543 0\n",
            "2853   0.14688372611999512 7\n",
            "2862   0.14417243003845215 7\n",
            "2871   0.14412617683410645 0\n",
            "2880   0.14316916465759277 4\n",
            "2889   0.10257911682128906 4\n",
            "2898   0.10468578338623047 6\n",
            "2907   0.10048913955688477 6\n",
            "2916   0.10059618949890137 6\n",
            "2925   0.09786248207092285 2\n",
            "2934   0.09970879554748535 2\n",
            "2943   0.09893655776977539 2\n",
            "2952   0.1027376651763916 2\n",
            "2961   0.09659767150878906 6\n",
            "2970   0.10040569305419922 6\n",
            "2979   0.0981595516204834 6\n",
            "2988   0.09859514236450195 4\n",
            "2997   0.10339736938476562 4\n",
            "3006   0.0968174934387207 7\n",
            "3015   0.09891915321350098 0\n",
            "3024   0.0966196060180664 0\n",
            "3033   0.09919857978820801 7\n",
            "3042   0.09891557693481445 0\n",
            "3051   0.15439844131469727 7\n",
            "3060   0.15755367279052734 7\n",
            "3069   0.1474931240081787 5\n",
            "3078   0.1755368709564209 0\n",
            "3087   0.14809823036193848 7\n",
            "3096   0.1544816493988037 4\n",
            "3105   0.15949296951293945 4\n",
            "3114   0.16396522521972656 4\n",
            "3123   0.15426373481750488 4\n",
            "3132   0.20126628875732422 6\n",
            "3141   0.2022862434387207 6\n",
            "3150   0.19404888153076172 2\n",
            "3159   0.2046976089477539 2\n",
            "3168   0.20104098320007324 2\n",
            "3177   0.1956007480621338 2\n",
            "3186   0.19727301597595215 6\n",
            "3195   0.20170998573303223 6\n",
            "3204   0.20383000373840332 4\n",
            "3213   0.20436596870422363 4\n",
            "3222   0.1861867904663086 4\n",
            "3231   0.16004133224487305 7\n",
            "3240   0.15297961235046387 4\n",
            "3249   0.15525221824645996 5\n",
            "3258   0.1759040355682373 7\n",
            "3267   0.15612316131591797 0\n",
            "3276   0.15616464614868164 0\n",
            "3285   0.16302013397216797 7\n",
            "3294   0.16492867469787598 7\n",
            "3303   0.15286684036254883 7\n",
            "3312   0.15335464477539062 5\n",
            "3321   0.15806841850280762 0\n",
            "3330   0.153977632522583 0\n",
            "3339   0.152968168258667 7\n",
            "3348   0.15457797050476074 4\n",
            "3357   0.15930962562561035 4\n",
            "3366   0.15319514274597168 0\n",
            "3375   0.15318918228149414 6\n",
            "3384   0.1530005931854248 6\n",
            "3393   0.1504814624786377 6\n",
            "3402   0.1501617431640625 2\n",
            "3411   0.15887045860290527 2\n",
            "3420   0.1602025032043457 2\n",
            "3429   0.14787507057189941 2\n",
            "3438   0.15295100212097168 6\n",
            "3447   0.15474247932434082 6\n",
            "3456   0.15382885932922363 6\n",
            "3465   0.15337300300598145 0\n",
            "3474   0.1504383087158203 4\n",
            "3483   0.14909768104553223 4\n",
            "3492   0.15713143348693848 0\n",
            "3501   0.1544816493988037 7\n",
            "3510   0.15431880950927734 5\n",
            "3519   0.1627953052520752 7\n",
            "3528   0.15596890449523926 0\n",
            "3537   0.15537166595458984 7\n",
            "3546   0.15618681907653809 7\n",
            "3555   0.15532159805297852 0\n",
            "3564   0.15678071975708008 0\n",
            "3573   0.15452957153320312 7\n",
            "3582   0.15410876274108887 4\n",
            "3591   0.15607333183288574 4\n",
            "3600   0.1538398265838623 6\n",
            "3609   0.15520286560058594 6\n",
            "3618   0.1577761173248291 2\n",
            "3627   0.15758991241455078 2\n",
            "3636   0.15655207633972168 2\n",
            "3645   0.16112375259399414 2\n",
            "3654   0.15915608406066895 2\n",
            "3663   0.16573596000671387 2\n",
            "3672   0.1606578826904297 6\n",
            "3681   0.1565253734588623 6\n",
            "3690   0.16253662109375 4\n",
            "3699   0.15549111366271973 4\n",
            "3708   0.15640807151794434 0\n",
            "3717   0.16266822814941406 7\n",
            "3726   0.1486804485321045 7\n",
            "3735   0.14865612983703613 0\n",
            "3744   0.15181803703308105 1\n",
            "3753   0.1608574390411377 1\n",
            "3762   0.15514636039733887 1\n",
            "3771   0.17284703254699707 1\n",
            "3780   0.1645646095275879 1\n",
            "3789   0.17909741401672363 1\n",
            "3798   0.20864152908325195 1\n",
            "3807   0.1664266586303711 1\n",
            "3816   0.16128897666931152 1\n",
            "3825   0.16748881340026855 1\n",
            "3834   0.16227197647094727 1\n",
            "3843   0.1605081558227539 1\n",
            "3852   0.1539750099182129 1\n",
            "3861   0.15656042098999023 1\n",
            "3870   0.15484142303466797 1\n",
            "3879   0.14804553985595703 1\n",
            "3888   0.17479538917541504 1\n",
            "3897   0.1435105800628662 1\n",
            "3906   0.13953828811645508 1\n",
            "3915   0.14180493354797363 1\n",
            "3924   0.14720582962036133 1\n",
            "3933   0.14289283752441406 1\n",
            "3942   0.14503860473632812 1\n",
            "3951   0.13502907752990723 1\n",
            "3960   0.13679742813110352 1\n",
            "3969   0.1428050994873047 1\n",
            "3978   0.1432662010192871 1\n",
            "3987   0.15007925033569336 1\n",
            "3996   0.14542198181152344 1\n",
            "4005   0.15131306648254395 1\n",
            "4014   0.14682269096374512 1\n",
            "4023   0.15229439735412598 1\n",
            "4032   0.13612890243530273 1\n",
            "4041   0.14656567573547363 1\n",
            "4050   0.14328527450561523 1\n",
            "4059   0.14613914489746094 1\n",
            "4068   0.15155911445617676 1\n",
            "4077   0.14692378044128418 1\n",
            "4086   0.13708114624023438 1\n",
            "4095   0.1494896411895752 1\n",
            "4104   0.14987802505493164 1\n",
            "4113   0.15616083145141602 1\n",
            "4122   0.15627598762512207 1\n",
            "4131   0.1579751968383789 1\n",
            "4140   0.1577150821685791 1\n",
            "4149   0.14548921585083008 1\n",
            "4158   0.1505885124206543 1\n",
            "4167   0.14444565773010254 1\n",
            "4176   0.14288735389709473 1\n",
            "4185   0.14475560188293457 1\n",
            "4194   0.15000629425048828 1\n",
            "4203   0.15723919868469238 1\n",
            "4212   0.15815114974975586 1\n",
            "4221   0.16285109519958496 1\n",
            "4230   0.15974855422973633 1\n",
            "4239   0.16314363479614258 1\n",
            "4248   0.15690922737121582 1\n",
            "4257   0.15755772590637207 1\n",
            "4266   0.16129064559936523 1\n",
            "4275   0.1477205753326416 1\n",
            "4284   0.14344143867492676 3\n",
            "4293   0.15142607688903809 3\n",
            "4302   0.14635276794433594 3\n",
            "4311   0.17332673072814941 3\n",
            "4320   0.15180277824401855 3\n",
            "4329   0.15173125267028809 3\n",
            "4338   0.15194272994995117 3\n",
            "4347   0.16633319854736328 3\n",
            "4356   0.14407062530517578 3\n",
            "4365   0.15064167976379395 3\n",
            "4374   0.1515064239501953 3\n",
            "4383   0.14965581893920898 3\n",
            "4392   0.14705348014831543 3\n",
            "4401   0.14579057693481445 3\n",
            "4410   0.1477816104888916 3\n",
            "4419   0.14806771278381348 3\n",
            "4428   0.14704513549804688 3\n",
            "4437   0.1513979434967041 3\n",
            "4446   0.17025136947631836 3\n",
            "4455   0.14657211303710938 3\n",
            "4464   0.14938998222351074 3\n",
            "4473   0.14435911178588867 3\n",
            "4482   0.14804387092590332 3\n",
            "4491   0.14981746673583984 3\n",
            "4500   0.14910507202148438 3\n",
            "4509   0.15532898902893066 3\n",
            "4518   0.15274763107299805 3\n",
            "4527   0.14971470832824707 3\n",
            "4536   0.15094327926635742 3\n",
            "4545   0.10718894004821777 3\n",
            "4554   0.09696006774902344 3\n",
            "4563   0.10112237930297852 3\n",
            "4572   0.10085701942443848 3\n",
            "4581   0.09657597541809082 3\n",
            "4590   0.09815692901611328 3\n",
            "4599   0.09576869010925293 3\n",
            "4608   0.09688234329223633 3\n",
            "4617   0.10219621658325195 3\n",
            "4626   0.10583901405334473 3\n",
            "4635   0.10241246223449707 3\n",
            "4644   0.10110735893249512 3\n",
            "4653   0.10024905204772949 3\n",
            "4662   0.10152292251586914 3\n",
            "4671   0.0992431640625 3\n",
            "4680   0.1011350154876709 3\n",
            "4689   0.10083556175231934 3\n",
            "4698   0.10077714920043945 3\n",
            "4707   0.10469746589660645 3\n",
            "4716   0.14794564247131348 3\n",
            "4725   0.20634937286376953 3\n",
            "4734   0.15593838691711426 3\n",
            "4743   0.16808533668518066 3\n",
            "4752   0.16078639030456543 3\n",
            "4761   0.15049242973327637 3\n",
            "4770   0.1574690341949463 3\n",
            "4779   0.15179038047790527 3\n",
            "4788   0.14226245880126953 3\n",
            "4797   0.15090298652648926 3\n",
            "4806   0.14402198791503906 3\n",
            "4815   0.15615034103393555 3\n",
            "4824   0.14567971229553223 3\n",
            "4833   0.14960670471191406 3\n",
            "4842   0.13916254043579102 3\n",
            "4851   0.1458570957183838 3\n",
            "4860   0.15745186805725098 3\n",
            "(4860, 224, 224, 3) (4860, 8)\n",
            "Train on 3888 samples, validate on 972 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsgdNJ_zQqoF",
        "colab_type": "code",
        "outputId": "c2f60e3a-e4da-4262-ad2b-736759ee43a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Flatten,Dense,Input,AveragePooling2D\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import numpy\n",
        "from keras.applications import mobilenet \n",
        "from skimage import img_as_ubyte\n",
        "import skimage.transform\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "image = cv2.imread('/content/drive/My Drive/SLR/Images/H_21.jpg')\n",
        "#image=np.array(image)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "image = img_as_ubyte(skimage.transform.resize(image, (224, 224)))\n",
        "\n",
        "#image.resize(1,224,224,3)\n",
        "image = image.astype(np.float32)\n",
        "image=image/255\n",
        "#print(len(model2.layers))\n",
        "\n",
        "model2 = tf.keras.models.load_model(\"/content/my_model.h5\")\n",
        "\n",
        "image = image.reshape(1,224,224,3)\n",
        "\n",
        "pred = model2.predict(image)\n",
        "print(pred,np.argmax(pred))\n",
        "    \n",
        "  \n",
        "  #print(chr(pred[i]+65),chr(np.argmax(y_test[i])+65))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[3.3415600e-24 1.9920531e-22 2.1985350e-14 3.6958926e-24 6.9170402e-32\n",
            "  2.1366334e-20 5.2140038e-03 9.9478602e-01]] 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS2Yloay0nDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}